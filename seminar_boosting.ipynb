{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Алгоритмы интеллектуальной обработки больших объемов данных\n",
    "## Семинар, учимся настраивать параметры бустинга\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберем, например, xgboost:\n",
    "https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "\n",
    "https://xgboost.readthedocs.io/en/latest/python/python_api.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметры можно разделить на группы. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Общие параметры алгоритма, например, тип бустинга\n",
    "\n",
    "2) Параметры обучения бустинга, например число итераций, learning rate\n",
    "\n",
    "3) Параметры построения деревьев, например, глубина, минимальное число элементов в листе\n",
    "\n",
    "4) Параметры стохастики, subsample, colsample\n",
    "\n",
    "5) Регуляриазция, например, lambda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ПРАВИЛЬНОГО алгоритма для тюнинга нет, вот один из вариантов, которые часто используют на практике."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Возьмите дефолтное параметры. Поставьте нужные loss и eval функции.  Измените максимальное  число деревьев, чтобы было не долго делать эксперименты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Подберите для него alpha, чтобы алгоритм не переобучался"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Подберите параметры сложности дерева, например,  глубину, мин число объектов в листах, мин значение для расщепления и тд"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Подберите паарметры стохастики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Подберите регуляризацию"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Уменьшите leraning rate и обучите максимальное число деревьев, чтобы не переобучалось"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте настроим параметры бустинга на нашем конкурсе!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28026\n"
     ]
    }
   ],
   "source": [
    "doc_to_title = {}\n",
    "with open('data/docs_titles.tsv') as f:\n",
    "    for num_line, line in enumerate(f):\n",
    "        if num_line == 0:\n",
    "            continue\n",
    "        data = line.strip().split('\\t', 1)\n",
    "        doc_id = int(data[0])\n",
    "        if len(data) == 1:\n",
    "            title = ''\n",
    "        else:\n",
    "            title = data[1]\n",
    "        doc_to_title[doc_id] = title\n",
    "print (len(doc_to_title))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv('data/train_groups.csv')\n",
    "traingroups_titledata = {}\n",
    "for i in range(len(train_data)):\n",
    "    new_doc = train_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    target = new_doc['target']\n",
    "    title = doc_to_title[doc_id]\n",
    "    if doc_group not in traingroups_titledata:\n",
    "        traingroups_titledata[doc_group] = []\n",
    "    traingroups_titledata[doc_group].append((doc_id, title, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 15) (11690,) (11690,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "y_train = []\n",
    "X_train = []\n",
    "groups_train = []\n",
    "for new_group in traingroups_titledata:\n",
    "    docs = traingroups_titledata[new_group]\n",
    "    for k, (doc_id, title, target_id) in enumerate(docs):\n",
    "        y_train.append(target_id)\n",
    "        groups_train.append(new_group)\n",
    "        all_dist = []\n",
    "        words = set(title.strip().split())\n",
    "        for j in range(0, len(docs)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j, target_j = docs[j]\n",
    "            words_j = set(title_j.strip().split())\n",
    "            all_dist.append(len(words.intersection(words_j)))\n",
    "        X_train.append(sorted(all_dist, reverse=True)[0:15]    )\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "groups_train = np.array(groups_train)\n",
    "print (X_train.shape, y_train.shape, groups_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подберите размер батча для обучения. Линейная модель не должна учиться дольше нескольких минут. \n",
    "\n",
    "Не забывайте использовать скейлер!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import random\n",
    "clf = xgb.XGBClassifier()\n",
    "clf = lgb.LGBMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_train = groups_train.reshape(-1, 1)\n",
    "uniq_for_shuffle = np.unique(groups_train)\n",
    "random.shuffle(uniq_for_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_learner(**params):\n",
    "    scores = []\n",
    "    splits = 5\n",
    "    val_gr_count = round(len(uniq_for_shuffle)/splits)\n",
    "\n",
    "    for i in range(splits):\n",
    "        val_groups = uniq_for_shuffle[i*val_gr_count:(i+1)*val_gr_count]\n",
    "        val_idx = np.where(groups_train == val_groups)[0]\n",
    "        train_groups = np.hstack((uniq_for_shuffle[:i*val_gr_count], uniq_for_shuffle[(i+1)*val_gr_count:]))\n",
    "        train_idx = np.where(groups_train == train_groups)[0]\n",
    "\n",
    "        X_tr = X_train[train_idx]\n",
    "        y_tr = y_train[train_idx]\n",
    "        X_val = X_train[val_idx]\n",
    "        y_val = y_train[val_idx]\n",
    "        \n",
    "        xgbc = xgb.XGBClassifier(**params)\n",
    "        xgbc.fit(X_tr, y_tr)\n",
    "        y_pred = xgbc.predict(X_val)\n",
    "        score = f1_score(y_val, y_pred)\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подберем learning rate для n_estimators=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      "\n",
      "  1%|          | 1/100 [00:00<01:02,  1.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 2/100 [00:01<01:01,  1.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 3/100 [00:01<01:02,  1.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 4/100 [00:02<01:00,  1.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 5/100 [00:03<00:58,  1.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 6/100 [00:03<00:56,  1.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 7/100 [00:04<00:56,  1.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 8/100 [00:04<00:55,  1.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 9/100 [00:05<00:58,  1.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 10/100 [00:06<01:00,  1.48it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 11/100 [00:06<00:57,  1.54it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 12/100 [00:07<00:55,  1.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 13/100 [00:08<00:56,  1.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 14/100 [00:09<01:00,  1.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 15/100 [00:10<01:10,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 16/100 [00:10<01:05,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 17/100 [00:11<01:00,  1.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 18/100 [00:12<00:55,  1.48it/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 19/100 [00:12<00:52,  1.54it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 20/100 [00:13<00:54,  1.48it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 21/100 [00:14<00:54,  1.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 22/100 [00:14<00:51,  1.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 23/100 [00:15<00:48,  1.58it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▍       | 24/100 [00:15<00:48,  1.58it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 25/100 [00:16<00:49,  1.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 26/100 [00:17<00:48,  1.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 27/100 [00:17<00:46,  1.58it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 28/100 [00:18<00:43,  1.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 29/100 [00:18<00:43,  1.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 30/100 [00:19<00:44,  1.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███       | 31/100 [00:20<00:44,  1.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 32/100 [00:21<00:46,  1.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 33/100 [00:21<00:43,  1.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 34/100 [00:22<00:41,  1.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▌      | 35/100 [00:22<00:40,  1.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▌      | 36/100 [00:23<00:45,  1.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 37/100 [00:24<00:43,  1.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 38/100 [00:25<00:51,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▉      | 39/100 [00:26<00:46,  1.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 40/100 [00:26<00:41,  1.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 41/100 [00:27<00:40,  1.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 42/100 [00:27<00:38,  1.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 43/100 [00:28<00:39,  1.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▍     | 44/100 [00:29<00:39,  1.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▌     | 45/100 [00:30<00:38,  1.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 46/100 [00:30<00:37,  1.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 47/100 [00:31<00:36,  1.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 48/100 [00:32<00:37,  1.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████▉     | 49/100 [00:33<00:36,  1.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 50/100 [00:33<00:33,  1.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████     | 51/100 [00:34<00:32,  1.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 52/100 [00:34<00:32,  1.47it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 53/100 [00:35<00:31,  1.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▍    | 54/100 [00:36<00:32,  1.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▌    | 55/100 [00:37<00:33,  1.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▌    | 56/100 [00:38<00:33,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 57/100 [00:38<00:32,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 58/100 [00:39<00:30,  1.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▉    | 59/100 [00:40<00:28,  1.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 60/100 [00:40<00:26,  1.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████    | 61/100 [00:41<00:25,  1.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 62/100 [00:41<00:25,  1.48it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 63/100 [00:42<00:25,  1.48it/s]\u001b[A\u001b[A\n",
      "\n",
      " 64%|██████▍   | 64/100 [00:43<00:24,  1.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▌   | 65/100 [00:44<00:26,  1.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▌   | 66/100 [00:45<00:29,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 67/100 [00:46<00:26,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 68/100 [00:46<00:23,  1.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▉   | 69/100 [00:47<00:21,  1.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 70/100 [00:47<00:19,  1.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 71/100 [00:48<00:19,  1.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 72/100 [00:49<00:18,  1.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 73/100 [00:50<00:26,  1.01it/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████▍  | 74/100 [00:51<00:23,  1.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▌  | 75/100 [00:52<00:20,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▌  | 76/100 [00:52<00:18,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 77/100 [00:53<00:17,  1.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 78/100 [00:54<00:16,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▉  | 79/100 [00:55<00:16,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 80/100 [00:55<00:14,  1.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████  | 81/100 [00:56<00:14,  1.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 82/100 [00:57<00:13,  1.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 83/100 [00:58<00:13,  1.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████▍ | 84/100 [00:58<00:12,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▌ | 85/100 [00:59<00:11,  1.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████▌ | 86/100 [01:00<00:10,  1.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 87/100 [01:00<00:09,  1.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 88/100 [01:01<00:08,  1.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|████████▉ | 89/100 [01:02<00:07,  1.48it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 90/100 [01:02<00:06,  1.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|█████████ | 91/100 [01:03<00:05,  1.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 92/100 [01:04<00:05,  1.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 93/100 [01:04<00:04,  1.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 94/100 [01:05<00:03,  1.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 95%|█████████▌| 95/100 [01:06<00:03,  1.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████▌| 96/100 [01:06<00:02,  1.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 97/100 [01:07<00:01,  1.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████▊| 98/100 [01:08<00:01,  1.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████▉| 99/100 [01:08<00:00,  1.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 100/100 [01:09<00:00,  1.52it/s]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "learning_rate = np.linspace(0, 1, 100)\n",
    "best_score = 0\n",
    "best_eta = learning_rate[0]\n",
    "bar = tqdm(total=learning_rate.shape[0])\n",
    "\n",
    "for lr in learning_rate:\n",
    "    score = cross_val_learner(eta=lr, n_estimators=30)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_eta = lr\n",
    "    bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5525982501200873, 0.27272727272727276)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score, best_eta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Настроим параметры дерева"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_estimators': 30, 'eta': best_eta, 'scale_pos_weight': 2.5}\n",
    "best_score = 0\n",
    "gammas = np.logspace(-3, 2, 10)\n",
    "min_child_weights = np.logspace(-3, 2, 10)\n",
    "\n",
    "for i, g in enumerate(gammas):\n",
    "    for min_ch_w in min_child_weights:\n",
    "        for depth in range(1, 8):\n",
    "            score = cross_val_learner(max_depth=depth, gamma=g, min_child_weight=min_ch_w, **params)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_depth = depth\n",
    "                best_gamma = g\n",
    "                best_mcw = min_ch_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.62545778149741, 7, 0.003593813663804626, 100.0)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score, best_depth, best_gamma, best_mcw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Параметры стохастики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_estimators': 30, 'eta': best_eta, 'scale_pos_weight': 2.5, 'max_depth': 1}\n",
    "best_score = 0\n",
    "subsamples = np.linspace(0.25, 1, 4)\n",
    "colsample_bytrees = np.linspace(0.25, 1, 4)\n",
    "colsample_bynodes = np.linspace(0.25, 1, 4)\n",
    "\n",
    "for subs in subsamples:\n",
    "    for colt in colsample_bytrees:\n",
    "        for coln in colsample_bynodes:\n",
    "            score = cross_val_learner(subsample=subs, colsample_bytree=colt,\\\n",
    "                                      colsample_bynode=coln, **params)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_subs = subs\n",
    "                best_colt = colt\n",
    "                best_coln = coln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6241772064068352, 0.25, 1.0, 1.0)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score, best_subs, best_colt, best_coln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Регуляризация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_estimators': 30, 'eta': best_eta, 'scale_pos_weight': 2.5,\\\n",
    "         'max_depth': 1, 'subsample': 0.25}\n",
    "best_score = 0\n",
    "lambdas = np.logspace(-2, 2, 40)\n",
    "\n",
    "for l in lambdas:\n",
    "    score = cross_val_learner(reg_lambda=l, **params)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_lambda = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.625096291025729, 2.2854638641349907)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score, best_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сделаем предикт на тесте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_data = pd.read_csv('data/test_groups.csv')\n",
    "testgroups_titledata = {}\n",
    "for i in range(len(test_data)):\n",
    "    new_doc = test_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    title = doc_to_title[doc_id]\n",
    "    if doc_group not in testgroups_titledata:\n",
    "        testgroups_titledata[doc_group] = []\n",
    "    testgroups_titledata[doc_group].append((doc_id, title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16627, 15) (16627,)\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "groups_test = []\n",
    "for new_group in testgroups_titledata:\n",
    "    docs = testgroups_titledata[new_group]\n",
    "    for k, (doc_id, title) in enumerate(docs):\n",
    "        groups_test.append(new_group)\n",
    "        all_dist = []\n",
    "        words = set(title.strip().split())\n",
    "        for j in range(0, len(docs)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j = docs[j]\n",
    "            words_j = set(title_j.strip().split())\n",
    "            all_dist.append(len(words.intersection(words_j)))\n",
    "        X_test.append(sorted(all_dist, reverse=True)[0:15]    )\n",
    "X_test = np.array(X_test)\n",
    "groups_test = np.array(groups_test)\n",
    "print (X_test.shape, groups_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_estimators': 30, 'eta': best_eta, 'scale_pos_weight': 2.5,\\\n",
    "         'max_depth': 1, 'subsample': 0.25, 'reg_lambda': best_lambda}\n",
    "xgbc = xgb.XGBClassifier(**params)\n",
    "xgbc.fit(X_train, y_train)\n",
    "y_test = xgbc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_test = y_test.shape[0]\n",
    "idx = np.arange(11691, 11691+len_test)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "idx = idx.reshape(-1, 1)\n",
    "y_test = np.hstack((idx, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.DataFrame(y_test, columns=('pair_id', 'target'))\n",
    "submit.to_csv('xgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача: обучите на этих фичах (число фичей рассатриваем как гипераматр) бустинг, подберите параметры, засабмитьте на кагл. Побейте 0.62 паблик скор за сутки.  Удачи!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "402px",
    "width": "253px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
